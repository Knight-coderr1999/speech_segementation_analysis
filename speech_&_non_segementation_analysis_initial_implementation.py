# -*- coding: utf-8 -*-
"""Speech & Non Segementation Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aqivjq4jshPL6uhGlvmxpwZT-io-MPK3
"""

!pip install speechbrain
!pip install pydub

# to convert flac or webm into wav
from pydub import AudioSegment
import os

def convert_to_wav(input_file):
    # Extract file extension
    file_ext = os.path.splitext(input_file)[1].lower()

    # Define the output file path
    output_file = os.path.splitext(input_file)[0] + ".wav"

    # Check format and convert accordingly
    if file_ext in [".webm", ".flac"]:
        audio = AudioSegment.from_file(input_file, format=file_ext[1:])  # Remove dot from extension
        audio.export(output_file, format="wav")
        print(f"Conversion successful: {output_file}")
    else:
        print("Unsupported file format. Only WEBM and FLAC are allowed.")

    return output_file

# Example usage
input_audio = "sample2.flac"  # Change to sample.webm if needed
converted_audio = convert_to_wav(input_audio)

# from pydub import AudioSegment

# audio = AudioSegment.from_file("sample-9.webm", format="webm")
# audio.export("sample-9.wav", format="wav")

from speechbrain.inference.separation import SepformerSeparation as separator
import torchaudio
from IPython.display import Audio
model = separator.from_hparams(source="speechbrain/sepformer-wham16k-enhancement", savedir='pretrained_models/sepformer-wham16k-enhancement')
audio_sources = model.separate_file(path='sample2.wav')
torchaudio.save("converted_audio.wav", audio_sources[:, :, 0], 16000)

# from speechbrain.inference.separation import SepformerSeparation as separator
# import torchaudio
# import torchaudio.transforms as transforms

# # Load and preprocess the audio
# audio_path = "sample1.wav"
# waveform, sample_rate = torchaudio.load(audio_path)

# # Convert to mono if stereo
# if waveform.shape[0] > 1:
#     waveform = waveform.mean(dim=0, keepdim=True)

# # Normalize audio
# waveform = waveform / waveform.abs().max()

# # Save preprocessed audio
# preprocessed_path = "preprocessed.wav"
# torchaudio.save(preprocessed_path, waveform, sample_rate)

# # Load SpeechBrain model (using a more robust model for reverberation and noise)
# model = separator.from_hparams(source="speechbrain/sepformer-whamr-enhancement",
#                                savedir="pretrained_models/sepformer-whamr-enhancement")

# # Perform separation
# audio_sources = model.separate_file(path=preprocessed_path)

# # Determine the number of sources detected
# num_sources = audio_sources.shape[-1]  # Typically 2 (voice + drums)

# # Save separated outputs dynamically
# for i in range(num_sources):
#     output_path = f"separated_source_{i+1}.wav"
#     torchaudio.save(output_path, audio_sources[:, :, i], sample_rate)
#     print(f"Saved: {output_path}")



